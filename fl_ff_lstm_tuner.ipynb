{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b6a0b27"
      },
      "source": [
        "## LSTM Model Tuning using Keras Tuner"
      ],
      "id": "8b6a0b27"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "707ffc16"
      },
      "source": [
        "This notebook uses the Keras tuner to optimize a LSTM model. The model is used in the federated learning notebook also in this project."
      ],
      "id": "707ffc16"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75855b0f"
      },
      "source": [
        "The data used is from the paper: *Framework for Creating Forest Fire Ignition Prediction Datasets.* Each row represents meteorological data at a geographical location at a specific time. TODO: Add table example."
      ],
      "id": "75855b0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHrXNUBl2pAJ"
      },
      "source": [
        "Much of the code used in this notebook is based on the Keras tuner code examples located [here](https://www.tensorflow.org/tutorials/keras/keras_tuner) and the Keras timeseries tutorials located [here](https://keras.io/examples/timeseries/).\n",
        "The code below is very much a work in progress."
      ],
      "id": "BHrXNUBl2pAJ"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dRK69YrCpyOz"
      },
      "outputs": [],
      "source": [
        "# if this file is being used in colab set to 1 otherwise 0\n",
        "using_colab = 1"
      ],
      "id": "dRK69YrCpyOz"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48aKzyReXGuh",
        "outputId": "7ffab08f-aaa6-46be-b7a5-f908c9ee45f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# if on Colab, load data from drive\n",
        "if (using_colab == 1):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "id": "48aKzyReXGuh"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "0bef8a61-ba13-4df2-8498-e82cc8ff9775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "# if on Colab, what kind of runtime?\n",
        "if (using_colab ==1):\n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')"
      ],
      "id": "V1G82GuO-tez"
    },
    {
      "cell_type": "code",
      "source": [
        "# install Keras tuner library if on Colab\n",
        "!pip install -q keras_tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiwsB7kWNzBC",
        "outputId": "29878883-ff54-4f8b-963a-7d0c0f7e875d"
      },
      "id": "oiwsB7kWNzBC",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aa962694"
      },
      "outputs": [],
      "source": [
        "# load libraries\n",
        "# TODO: get rid of libraries that aren't needed\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "import gc\n",
        "import datetime\n",
        "\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import Model\n",
        "\n",
        "import keras_tuner as kt"
      ],
      "id": "aa962694"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4f6f82c",
        "outputId": "fd3390a5-a30e-477b-b8c5-d24b2e48f217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on CPU    using TensorFlow 2.12.0, Keras tuner 1.3.5, and Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "# overall environment settings\n",
        "\n",
        "# Make TensorFlow logs less verbose\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# Training on GPU or CPU?\n",
        "# tf.config.set_visible_devices([], 'GPU')\n",
        "python_version = !python --version\n",
        "print(\n",
        "    f\"Training on {'GPU' if tf.config.get_visible_devices('GPU') else 'CPU'}\\\n",
        "    using TensorFlow {tf.__version__}, Keras tuner {kt.__version__}, and {python_version[0]}\"\n",
        ")"
      ],
      "id": "a4f6f82c"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2ebbef88"
      },
      "outputs": [],
      "source": [
        "#global variables\n",
        "\n",
        "ml_type = 0 # classic ML = 0, federated ML w/ centralized evaluation = 1, federated ML w/ federated eval = 2\n",
        "\n",
        "cid = str(0) # preliminary client id\n",
        "\n",
        "master_path = \"/content/drive/MyDrive/Colab Notebooks/FF/\"\n",
        "federated_path = master_path + \"data/24_clients/\"\n",
        "centralized_path = master_path + \"data/01_clients/\"\n",
        "results_path = master_path + \"history/\"\n",
        "temp_path = master_path + \"history_temp/\"\n",
        "log_dir = master_path + \"tensorflow_logs/\" + cid + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "#master_path = <path to your master directory for this simulation>\n",
        "#federated_path = <path to your client datasets>\n",
        "#centralized_path = <path to your server dataset\n",
        "#results_path = <path to where you want to store results>\n",
        "#log_dir = <path to where you want to store Tensorflow logs> + cid + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "downsample_test_set = 0 # 0 if test set not downsampled, 1 otherwise\n",
        "\n",
        "sequence_len = 120 # 5 days * 24 hours\n",
        "past_len = sequence_len\n",
        "future_len = 24 # 1*24 hours\n",
        "sampling_rate = 1 # in time series conversion use every row (hour) in loaded datasets\n",
        "sequence_stride = 1 # in time series conversion each series is this far apart\n",
        "\n",
        "\n"
      ],
      "id": "2ebbef88"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2951ebb8"
      },
      "source": [
        "## Data loading functions\n",
        "The data loading functions are from the federated learning portion of this project and not optimized for this notebook."
      ],
      "id": "2951ebb8"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "900620b0"
      },
      "outputs": [],
      "source": [
        "def normalize_data(x):\n",
        "    \"\"\"Normalizes the data of an array by column.\n",
        "    Shifts and scales inputs into a distribution centered around 0\n",
        "    with standard deviation 1.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: NDarray\n",
        "        An array of feature values.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features_normalized : NDarray\n",
        "        The original array, but normalized.\n",
        "    \"\"\"\n",
        "    data = x\n",
        "    layer = layers.Normalization()\n",
        "    layer.adapt(data)\n",
        "    features_normalized = layer(data)\n",
        "    return features_normalized"
      ],
      "id": "900620b0"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "c91e58dc"
      },
      "outputs": [],
      "source": [
        "def mask_create(x):\n",
        "    \"\"\"Finds the class count of the input array and creates a mask that can be used\n",
        "    to randomly downsample an array of labels so that the number of\n",
        "    negative labels = the number of positive labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: NDarray\n",
        "        An array of feature values.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features_normalized : NDarray\n",
        "        A masked version of the input array.\n",
        "    \"\"\"\n",
        "    mask_length = x.shape[0]\n",
        "    mask = tf.reshape(x, [mask_length])\n",
        "    y, idx, class_count = tf.unique_with_counts(mask)\n",
        "    ignition_count = tf.get_static_value(class_count[1])\n",
        "    mask = mask.numpy()\n",
        "    count = 0\n",
        "    while count < ignition_count:\n",
        "        #rand_num = random.randint(0,mask_length)\n",
        "        rand_num = random.randint(1, mask_length-1)\n",
        "        if (mask[rand_num] == 0):\n",
        "            mask[rand_num] = 1\n",
        "            count += 1\n",
        "    return mask"
      ],
      "id": "c91e58dc"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "84c16d68"
      },
      "outputs": [],
      "source": [
        "def load_datasets(path: str):\n",
        "    \"\"\"Loads all the csv datasets in a folder.\n",
        "    The loaded data is divided into train, validation, and test sets.\n",
        "    The data is turned into time series data\n",
        "    All the data is normalized.\n",
        "    Train and validation datasets are downsampled.\n",
        "    TODO: divide this function into smaller functions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: string\n",
        "        The path to the dataset folder.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y : NDarrays\n",
        "        A masked version of the input array.\n",
        "    \"\"\"\n",
        "\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    val_x = []\n",
        "    val_y = []\n",
        "    test_x = []\n",
        "    test_y = []\n",
        "\n",
        "    #load data\n",
        "    for filename in glob.glob(os.path.join(path, '*.csv')):\n",
        "        print(\"\\nnow reading \" + filename + \"\\n\")\n",
        "        #read file\n",
        "        df = pd.read_csv(filename, index_col=[0])\n",
        "\n",
        "        df_train = df[(df['year'] < 2001)]\n",
        "        df_val = df[(df['year'] > 2001) & (df['year'] < 2012)]\n",
        "        df_test = df[(df['year'] >= 2012)]\n",
        "\n",
        "        features = ['stl2',\n",
        "                    't2m',\n",
        "                    'stl1',\n",
        "                    'stl3',\n",
        "                    'skt',\n",
        "                    'swvl1',\n",
        "                    'd2m',\n",
        "                    'swvl2'\n",
        "                    ] #this shoudn't be hard coded\n",
        "        train_features = df_train[features]\n",
        "        train_labels = df_train[[\"ignition\"]]\n",
        "        val_features = df_val[features]\n",
        "        val_labels = df_val[[\"ignition\"]]\n",
        "        test_features = df_test[features]\n",
        "        test_labels = df_test[[\"ignition\"]]\n",
        "        #convert to numpy\n",
        "        train_features = train_features.values\n",
        "        val_features = val_features.values\n",
        "        test_features = test_features.values\n",
        "\n",
        "        #normalize\n",
        "        train_features_normalize = normalize_data(train_features)\n",
        "        val_features_normalize = normalize_data(val_features)\n",
        "        test_features_normalize = normalize_data(test_features)\n",
        "\n",
        "        #we want to predict at a future point\n",
        "        #so we clip the length of the features plus the hours till the future point\n",
        "        start = past_len + future_len\n",
        "        train_labels = train_labels.iloc[start:].values\n",
        "        val_labels = val_labels.iloc[start:].values\n",
        "        test_labels = test_labels.iloc[start:].values\n",
        "\n",
        "        batch_size = 107856 #factor of 5136 (321 * 16)\n",
        "        #convert to time series data\n",
        "        train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "            train_features_normalize,\n",
        "            train_labels,\n",
        "            sampling_rate=sampling_rate,\n",
        "            sequence_length=sequence_len,\n",
        "            sequence_stride = sequence_stride,\n",
        "            shuffle=False,\n",
        "            batch_size=batch_size)\n",
        "\n",
        "        val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "            val_features_normalize,\n",
        "            val_labels,\n",
        "            sampling_rate=sampling_rate,\n",
        "            sequence_length=sequence_len,\n",
        "            sequence_stride = sequence_stride,\n",
        "            shuffle=False,\n",
        "            batch_size=batch_size)\n",
        "\n",
        "        test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "            test_features_normalize,\n",
        "            test_labels,\n",
        "            sampling_rate=sampling_rate,\n",
        "            sequence_length=sequence_len,\n",
        "            sequence_stride = sequence_stride,\n",
        "            shuffle=False,\n",
        "            batch_size=batch_size)\n",
        "\n",
        "        #for bookkeeping print out the shapes of the datasets\n",
        "        for train_features, train_labels in train_dataset:\n",
        "            print(\"train_dataset features shape:\", train_features.shape)\n",
        "            print(\"targets_dataset labels shape:\", train_labels.shape)\n",
        "            break\n",
        "\n",
        "        for val_features, val_labels in val_dataset:\n",
        "            print(\"\\nval_dataset features shape:\", val_features.shape)\n",
        "            print(\"val_dataset labels shape:\", val_labels.shape)\n",
        "            break\n",
        "\n",
        "        for test_features, test_labels in test_dataset:\n",
        "            print(\"\\ntest_dataset features shape:\", test_features.shape)\n",
        "            print(\"test_dataset labels shape:\", test_labels.shape)\n",
        "            break\n",
        "\n",
        "        # randomly downsample the data using masks\n",
        "        train_mask = mask_create(train_labels)\n",
        "        train_features_masked = tf.boolean_mask(train_features, train_mask)\n",
        "        train_labels_masked = tf.boolean_mask(train_labels, train_mask)\n",
        "\n",
        "        val_mask = mask_create(val_labels)\n",
        "        val_features_masked = tf.boolean_mask(val_features, val_mask)\n",
        "        val_labels_masked = tf.boolean_mask(val_labels, val_mask)\n",
        "\n",
        "        test_mask = mask_create(test_labels)\n",
        "        test_features_masked = tf.boolean_mask(test_features, test_mask)\n",
        "        test_labels_masked = tf.boolean_mask(test_labels, test_mask)\n",
        "\n",
        "        train_x.append(train_features_masked)\n",
        "        train_y.append(train_labels_masked)\n",
        "        val_x.append(val_features_masked)\n",
        "        val_y.append(val_labels_masked)\n",
        "        if (downsample_test_set == 1):\n",
        "            test_x.append(test_features_masked)\n",
        "            test_y.append(test_labels_masked)\n",
        "        else:\n",
        "            test_x.append(test_features)\n",
        "            test_y.append(test_labels)\n",
        "\n",
        "    print(\"\\nDone loading data.\\n\")\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
        "\n"
      ],
      "id": "84c16d68"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "54a0a5d0"
      },
      "outputs": [],
      "source": [
        "def get_value_count(x):\n",
        "    \"\"\"A helper function that returns the count of class labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: NDArray\n",
        "        An array with class labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    non_ignition_count, ignition_count : int\n",
        "        The counts of the ignition class.\n",
        "    \"\"\"\n",
        "    length = x[0].shape[0]\n",
        "    x = tf.reshape(x, [length])\n",
        "    y, idx, class_count = tf.unique_with_counts(x)\n",
        "    non_ignition_count = tf.get_static_value(class_count[0])\n",
        "    ignition_count = tf.get_static_value(class_count[1])\n",
        "    return non_ignition_count, ignition_count\n"
      ],
      "id": "54a0a5d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bae1c140"
      },
      "source": [
        "# Load data"
      ],
      "id": "bae1c140"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b71106bc",
        "outputId": "add54a1c-99e9-46ca-d390-c782d69057e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "now reading /content/drive/MyDrive/Colab Notebooks/FF/data/01_clients/dly_avg_1of1_50.csv\n",
            "\n",
            "train_dataset features shape: (107712, 120, 8)\n",
            "targets_dataset labels shape: (107712, 1)\n",
            "\n",
            "val_dataset features shape: (51216, 120, 8)\n",
            "val_dataset labels shape: (51216, 1)\n",
            "\n",
            "test_dataset features shape: (46080, 120, 8)\n",
            "test_dataset labels shape: (46080, 1)\n",
            "\n",
            "Done loading data.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load the dataset for centralized evaluation (for classic ml training and testing)\n",
        "trainloaders_x, trainloaders_y, valloaders_x, valloaders_y, testloaders_x, testloaders_y = load_datasets(centralized_path)"
      ],
      "id": "b71106bc"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5917c384",
        "outputId": "bcf3490c-8499-46ab-a8f1-2d2445891931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set nonignitions and ignitions are: (2811, 2811)\n",
            "Validation set nonignitions and ignitions are: (2534, 2534)\n",
            "Test set nonignitions and ignitions are: (44719, 1361)\n"
          ]
        }
      ],
      "source": [
        "# a quick check of the label count\n",
        "count = get_value_count(trainloaders_y)\n",
        "print(\"Train set nonignitions and ignitions are:\", count)\n",
        "count = get_value_count(valloaders_y)\n",
        "print(\"Validation set nonignitions and ignitions are:\", count)\n",
        "count = get_value_count(testloaders_y)\n",
        "print(\"Test set nonignitions and ignitions are:\", count)"
      ],
      "id": "5917c384"
    },
    {
      "cell_type": "code",
      "source": [
        "# define the metrics to be used\n",
        "\n",
        "METRICS = [\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.FalseNegatives(name='fn'),\n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    keras.metrics.AUC(name='auc'),\n",
        "    keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "    keras.metrics.SensitivityAtSpecificity(0.5, name='sensitivity'),\n",
        "    keras.metrics.SpecificityAtSensitivity(0.5, name='specificity')\n",
        "    #keras.metrics.F1Score(name='f1_score'),#only available with nightly build\n",
        "]"
      ],
      "metadata": {
        "id": "5o--RRak4dVx"
      },
      "id": "5o--RRak4dVx",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4eadefbb"
      },
      "outputs": [],
      "source": [
        "# define the metrics and model function and tuning parameters\n",
        "\n",
        "def make_model(hp, metrics=METRICS):\n",
        "    inputs = keras.Input(shape=(sequence_len, trainloaders_x[0].shape[2]))\n",
        "    x = layers.LSTM(hp.Choice('units', [8, 16, 32, 64, 128, 256]), activation='sigmoid', return_sequences=True)(inputs)\n",
        "    x = layers.LSTM(hp.Choice('units', [8, 16, 32, 64, 128, 256]), activation='sigmoid', return_sequences=True)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(1, activation=hp.Choice(\"activation\", [\"sigmoid\", \"relu\", \"tanh\"]))(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # Tune the learning rate for the optimizer\n",
        "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics)\n",
        "\n",
        "    return model\n"
      ],
      "id": "4eadefbb"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tuner object\n",
        "tuner = kt.RandomSearch(\n",
        "    hypermodel=make_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=12,\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"tuning_test\",\n",
        ")"
      ],
      "metadata": {
        "id": "_rpQZCuVNfuZ"
      },
      "id": "_rpQZCuVNfuZ",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do a hyperparameter search using the tuner object\n",
        "tuner.search(trainloaders_x[0],\n",
        "             trainloaders_y[0],\n",
        "             epochs=5,\n",
        "             validation_data=(valloaders_x, valloaders_y))\n",
        "best_model = tuner.get_best_models()[0]"
      ],
      "metadata": {
        "id": "cueW5HqgOZae"
      },
      "id": "cueW5HqgOZae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 2 models.\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "# Build the model.\n",
        "# Needed for `Sequential` without specified `input_shape`.\n",
        "#best_model.build(input_shape=(None, 28, 28))\n",
        "best_model.summary()"
      ],
      "metadata": {
        "id": "EUeIGVWXCGMM"
      },
      "id": "EUeIGVWXCGMM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "TwIgaLRkCRSm"
      },
      "id": "TwIgaLRkCRSm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}